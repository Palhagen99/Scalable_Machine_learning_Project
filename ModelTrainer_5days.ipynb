{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Main idea: Fetch multiple datasets of indexes and stocks correlated news to that index. (News only go back one year)\n",
    "Then we train the model on the multiple datasets and test it on the last dataset.\n",
    "\n",
    "1. Fetch the stock data from Yahoo Finance\n",
    "2. Fetch the news data from NewsAPI\n",
    "3. Preprocess stock data. We will give the model open, close, high, low\n",
    "4. Preprocess news data to sentiment labels. then backfill the sentiment labels so that there are no missing values\n",
    "5. Merge the stock data and news data\n",
    "6. Train the model on the multiple datasets\n",
    "7. Test the model on the last dataset\n",
    "8. Evaluate the model\n",
    "9. Save the model\n",
    "10. Make predictions"
   ],
   "id": "51fb259759d3717c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:23:48.424128Z",
     "start_time": "2025-01-07T11:23:48.417146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Data fetching \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from pygooglenews import GoogleNews"
   ],
   "id": "a1ca616477abb63d",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:23:48.459760Z",
     "start_time": "2025-01-07T11:23:48.457131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "List_of_indexes = ['^GSPC', '^DJI', '^IXIC', '^RUT', '^FTSE', '^N225', '^HSI', '^GDAXI', '^OMX']\n",
    "#List_of_indexes = ['^GSPC', '^DJI', '^IXIC', '^RUT', '^FTSE', '^N225', '^HSI', '^GDAXI', '^OMX', 'XLE', 'AAPL','MSFT','GOOGL','BRK-A','JPM','JNJ','V','WMT','PG','UNH','MA','INTC','VZ','HD','DIS','MRK','KO','PFE','BAC','PEP','CSCO','CMCSA','NFLX','T','ADBE','ABT','XOM','CRM','TMO','ACN','CVX','NKE']\n",
    "\n",
    "\n",
    "#List_of_indexes = ['^GSPC', '^DJI', '^IXIC', '^RUT', '^FTSE', '^N225', '^HSI', '^GDAXI', '^OMX', 'XLE', 'AAPL','MSFT','AMZN','GOOGL','TSLA','BRK-A','JPM','JNJ','V','WMT','PG','UNH','MA','INTC','VZ','HD','DIS','MRK','KO','PFE','BAC','PEP','CSCO','CMCSA','NFLX','T','NVDA','ADBE','ABT','XOM','CRM','TMO','ACN','CVX','NKE','LLY','COST','ABBV','DHR','NEE','AVGO','MDT','QCOM','TXN','UNP','LIN','UPS','HON','SBUX','AMT','ORCL','LOW','IBM','MO','AMD','AMGN','C','CAT','BA','MMM','GE','GS','MS','FDX','PYPL','CHTR','AMAT','LMT']\n",
    "\n",
    "#List_of_indexes = ['^GSPC', '^DJI', '^IXIC', '^RUT', '^FTSE', '^N225', '^HSI', '^GDAXI', '^OMX', 'XLE', 'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'TSLA', 'BRK-A', 'JPM', 'JNJ', 'V', 'WMT', 'PG', 'UNH', 'MA', 'INTC', 'VZ', 'HD', 'DIS', 'MRK', 'KO', 'PFE', 'BAC', 'PEP', 'CSCO', 'CMCSA', 'NFLX', 'T', 'NVDA', 'ADBE', 'ABT', 'XOM', 'CRM', 'TMO', 'ACN', 'CVX', 'NKE', 'LLY', 'COST', 'ABBV', 'DHR', 'NEE', 'AVGO', 'MDT', 'QCOM', 'TXN', 'UNP', 'LIN', 'UPS', 'HON', 'SBUX', 'AMT', 'ORCL', 'LOW', 'IBM', 'MO', 'AMD', 'AMGN', 'C', 'CAT', 'BA', 'MMM', 'GE', 'GS', 'MS', 'FDX', 'PYPL', 'CHTR', 'AMAT', 'LMT', 'META', 'SNOW', 'PANW', 'SHOP', 'UBER', 'BP', 'CVX', 'SLB', 'COP', 'ISRG', 'GILD', 'BIIB', 'REGN', 'MCD', 'TGT', 'ROST', 'SCHW', 'BLK', 'AXP', 'BK', 'TSM', 'BABA', 'NTES', 'JD', 'TCEHY', 'NESN.SW', 'RACE', 'ASML', 'SAP.DE', 'PBR', 'VALE', 'ITUB', 'NIO', 'LI', 'RIVN', 'LCID', 'MU', 'ENPH', 'SEDG', 'FSLR', '^VIX', '^RUA', '^AEX', '^STOXX50E', '^BSESN', '^KS11', 'XLV','XLF', 'XLK', 'XLY', 'XLP', 'XLU', 'XLB', 'XLI']"
   ],
   "id": "d7591eef997267b5",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:23:49.271209Z",
     "start_time": "2025-01-07T11:23:48.479895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Fetch stock data\n",
    "stock_data = {}\n",
    "for index in List_of_indexes:\n",
    "    #Manually create the dataframe\n",
    "    collected_data = yf.download(index, period =\"1y\")\n",
    "    df = pd.DataFrame()\n",
    "    df['Open'] = collected_data['Open']\n",
    "    df['Close'] = collected_data['Close']\n",
    "    df['High'] = collected_data['High']\n",
    "    df['Low'] = collected_data['Low']\n",
    "    stock_data[index] = df\n",
    "    \n"
   ],
   "id": "ec59fe66828cea63",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:23:54.908581Z",
     "start_time": "2025-01-07T11:23:49.272918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Fetch news data\n",
    "news_data = {}\n",
    "gn = GoogleNews()\n",
    "for index in List_of_indexes:\n",
    "    news_data[index] = gn.search(index, when='1y')\n",
    "    print(\"Fetched news data for: \", index)\n",
    "    print(\"Number of articles: \", len(news_data[index]['entries']))\n",
    "    #Convert to dataframe\n",
    "    news_data[index] = pd.DataFrame(news_data[index]['entries'])\n",
    "\n",
    "\n"
   ],
   "id": "6b5fff789bb9d021",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched news data for:  ^GSPC\n",
      "Number of articles:  75\n",
      "Fetched news data for:  ^DJI\n",
      "Number of articles:  98\n",
      "Fetched news data for:  ^IXIC\n",
      "Number of articles:  100\n",
      "Fetched news data for:  ^RUT\n",
      "Number of articles:  96\n",
      "Fetched news data for:  ^FTSE\n",
      "Number of articles:  99\n",
      "Fetched news data for:  ^N225\n",
      "Number of articles:  56\n",
      "Fetched news data for:  ^HSI\n",
      "Number of articles:  100\n",
      "Fetched news data for:  ^GDAXI\n",
      "Number of articles:  15\n",
      "Fetched news data for:  ^OMX\n",
      "Number of articles:  99\n"
     ]
    }
   ],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:24:13.246616Z",
     "start_time": "2025-01-07T11:23:54.909584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Clean up the news data wich contains: title,title_detail,links,link,id,guidislink,published,published_parsed,summary,summary_detail,source,sub_articles\n",
    "#We will put the title through a sentiment analysis model and then put all none existing values as 0\n",
    "#Finbert model\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "#Test the model\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "news_sentiments = {}\n",
    "\n",
    "#Add date and sentiment to the dataframe\n",
    "for index in List_of_indexes:\n",
    "    news_sentiments[index] = []\n",
    "    print(\"Processing news data for: \", index)\n",
    "    for i in range(len(news_data[index])):\n",
    "        #print(news_data[index].iloc[i]['title'])\n",
    "        sentiment = classifier(news_data[index].iloc[i]['title'], top_k=None)\n",
    "        postive_score = sentiment[0]['score']\n",
    "        negative_score = sentiment[1]['score']\n",
    "        neutral_score = sentiment[2]['score']\n",
    "        #Convert date to dtype='datetime64[ns] so that we can later can merge the dataframes\n",
    "        reformmated_time_stamp = pd.to_datetime(news_data[index].iloc[i]['published']).date()\n",
    "        news_sentiments[index] = news_sentiments[index] + [{'Date': reformmated_time_stamp, 'positive_score': postive_score, 'negative_score': negative_score, 'neutral_score': neutral_score}]\n",
    "    news_sentiments[index] = pd.DataFrame(news_sentiments[index])"
   ],
   "id": "3d85188c30af133a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing news data for:  ^GSPC\n",
      "Processing news data for:  ^DJI\n",
      "Processing news data for:  ^IXIC\n",
      "Processing news data for:  ^RUT\n",
      "Processing news data for:  ^FTSE\n",
      "Processing news data for:  ^N225\n",
      "Processing news data for:  ^HSI\n",
      "Processing news data for:  ^GDAXI\n",
      "Processing news data for:  ^OMX\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:24:13.250908Z",
     "start_time": "2025-01-07T11:24:13.249062Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "df3589333c61468d",
   "outputs": [],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:24:13.285389Z",
     "start_time": "2025-01-07T11:24:13.251493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Average sentiment for days with multiple articles\n",
    "\n",
    "for index in List_of_indexes:\n",
    "    # Group by date and calculate mean sentiment scores\n",
    "    news_sentiments[index] = (\n",
    "        news_sentiments[index]\n",
    "        .groupby('Date')\n",
    "        .mean()\n",
    "        .fillna(0)  # Handle NaN values\n",
    "        .reset_index()  # Ensure 'date' becomes a column\n",
    "        .set_index('Date')  # Set 'date' as the index\n",
    "        .sort_index()  # Ensure the index is sorted\n",
    "    )\n",
    "    \n",
    "    \n",
    "# Define a common date range\n",
    "common_index = pd.date_range(\n",
    "    start=pd.Timestamp(min(pd.Timestamp(stock_data[index].index.min()), pd.Timestamp(news_sentiments[index].index.min()))),\n",
    "    end=pd.Timestamp(max(pd.Timestamp(stock_data[index].index.max()), pd.Timestamp(news_sentiments[index].index.max()))),\n",
    "    freq='D'  # Daily frequency\n",
    ")\n",
    "\n",
    "# Reindex both dataframes so they have the same index\n",
    "stock_data[index] = stock_data[index].reindex(common_index).fillna(-1)  # Fill missing stock data with 0\n",
    "news_sentiments[index] = news_sentiments[index].reindex(common_index).fillna(0)  # Fill missing sentiment data with 0\n",
    "\n",
    "# Combine stock data and news sentiment into one DataFrame\n",
    "combined_data = {}\n",
    "\n",
    "\n",
    "# Reindex both dataframes\n",
    "\n",
    "for index in List_of_indexes:\n",
    "    # Ensure stock_data and news_sentiments have compatible indices\n",
    "    stock_data[index].index = pd.to_datetime(stock_data[index].index).normalize()  # Normalize index to date only\n",
    "    news_sentiments[index].index = pd.to_datetime(news_sentiments[index].index).normalize()  # Normalize index to date only\n",
    "    \n",
    "    # Merge stock data and news data\n",
    "    combined_data[index] = pd.merge(\n",
    "        stock_data[index], \n",
    "        news_sentiments[index], \n",
    "        how='left', \n",
    "        left_index=True, \n",
    "        right_index=True\n",
    "    )\n",
    "    \n",
    "    #Drop all close values that are -1\n",
    "    combined_data[index] = combined_data[index][combined_data[index].Close != -1]\n",
    "    \n",
    "    #Fill all missing values with 0\n",
    "    combined_data[index] = combined_data[index].fillna(0)\n",
    "    \n"
   ],
   "id": "f13084b211fc42fe",
   "outputs": [],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:24:13.288548Z",
     "start_time": "2025-01-07T11:24:13.286347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Check size of the dataframes\n",
    "for index in List_of_indexes:\n",
    "    print(\"Size of dataframe for \", index, \" is: \", combined_data[index].shape)"
   ],
   "id": "5b58402c518b6b1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe for  ^GSPC  is:  (251, 7)\n",
      "Size of dataframe for  ^DJI  is:  (251, 7)\n",
      "Size of dataframe for  ^IXIC  is:  (251, 7)\n",
      "Size of dataframe for  ^RUT  is:  (251, 7)\n",
      "Size of dataframe for  ^FTSE  is:  (254, 7)\n",
      "Size of dataframe for  ^N225  is:  (245, 7)\n",
      "Size of dataframe for  ^HSI  is:  (246, 7)\n",
      "Size of dataframe for  ^GDAXI  is:  (254, 7)\n",
      "Size of dataframe for  ^OMX  is:  (250, 7)\n"
     ]
    }
   ],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:24:13.333995Z",
     "start_time": "2025-01-07T11:24:13.289265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Reformat the dataframes to be able to train the model\n",
    "#We will use the last 30 days as testa data and the rest as training data as the data comes from diffrent indexes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def create_sequences(input_data, output_data, timesteps, output_dim):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(input_data) - output_dim - timesteps):\n",
    "        x = input_data[i:(i + timesteps)]\n",
    "        y = output_data[i + timesteps:i + output_dim + timesteps]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "timesteps = 30\n",
    "input_dim = 7\n",
    "output_dim = 5 #Predict 5 days ahead\n",
    "\n",
    "\n",
    "# Initialize global scalers\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "# Gather all data for global scaling\n",
    "xs = []\n",
    "ys = []\n",
    "for index in List_of_indexes: \n",
    "    data = combined_data[index]\n",
    "    # Normalize data\n",
    "    input_features = data[['Open', 'Close', 'High', 'Low', 'positive_score', 'negative_score', 'neutral_score']].values\n",
    "    output_feature = data[['Close']].values\n",
    "    scaler = MinMaxScaler()\n",
    "    input_features_scaled = scaler.fit_transform(input_features)\n",
    "    output_feature_scaled = scaler.fit_transform(output_feature)\n",
    "    \n",
    "    # Create sequences\n",
    "    x, y = create_sequences(input_features_scaled, output_feature_scaled, timesteps, output_dim)\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    \n",
    "# Concatenate all sequences\n",
    "xs = np.concatenate(xs)\n",
    "ys = np.concatenate(ys)\n",
    "\n",
    "print(xs.shape, ys.shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(xs.shape, ys.shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.1, shuffle=False)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)\n"
   ],
   "id": "2cfabbc37bb7f0d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1938, 30, 7) (1938, 5, 1)\n",
      "(1938, 30, 7) (1938, 5, 1)\n"
     ]
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:24:13.481340Z",
     "start_time": "2025-01-07T11:24:13.334620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential() \n",
    "model.add(LSTM(256, input_shape=(timesteps, input_dim)))\n",
    "\n",
    "\n",
    "model.add(Dense(output_dim))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val), callbacks=[early_stopping])\n"
   ],
   "id": "2988eac415239f9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 12:24:13,382 WARNING: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_15\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 256)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[235], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39madd(LSTM(\u001B[38;5;241m256\u001B[39m, input_shape\u001B[38;5;241m=\u001B[39m(timesteps, input_dim)))\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#Add a another LSTM layer\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mLSTM\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39madd(Dense(output_dim))\n\u001B[1;32m     10\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.virtualenvs/Scalable_Machine_learning_Project/lib/python3.12/site-packages/keras/src/models/sequential.py:122\u001B[0m, in \u001B[0;36mSequential.add\u001B[0;34m(self, layer, rebuild)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers\u001B[38;5;241m.\u001B[39mappend(layer)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rebuild:\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_rebuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/Scalable_Machine_learning_Project/lib/python3.12/site-packages/keras/src/models/sequential.py:141\u001B[0m, in \u001B[0;36mSequential._maybe_rebuild\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers[\u001B[38;5;241m0\u001B[39m], InputLayer) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    140\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mbatch_shape\n\u001B[0;32m--> 141\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_shape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;66;03m# We can build the Sequential model if the first layer has the\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001B[39;00m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;66;03m# model.\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39minput_shape\n",
      "File \u001B[0;32m~/.virtualenvs/Scalable_Machine_learning_Project/lib/python3.12/site-packages/keras/src/layers/layer.py:226\u001B[0m, in \u001B[0;36mLayer.__new__.<locals>.build_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_open_name_scope():\n\u001B[1;32m    225\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_path \u001B[38;5;241m=\u001B[39m current_path()\n\u001B[0;32m--> 226\u001B[0m     \u001B[43moriginal_build_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;66;03m# Record build config.\u001B[39;00m\n\u001B[1;32m    228\u001B[0m signature \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(original_build_method)\n",
      "File \u001B[0;32m~/.virtualenvs/Scalable_Machine_learning_Project/lib/python3.12/site-packages/keras/src/models/sequential.py:187\u001B[0m, in \u001B[0;36mSequential.build\u001B[0;34m(self, input_shape)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_layers[\u001B[38;5;241m1\u001B[39m:]:\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 187\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m:\n\u001B[1;32m    189\u001B[0m         \u001B[38;5;66;03m# Can happen if shape inference is not implemented.\u001B[39;00m\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001B[39;00m\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/Scalable_Machine_learning_Project/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.virtualenvs/Scalable_Machine_learning_Project/lib/python3.12/site-packages/keras/src/layers/input_spec.py:186\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[0;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m spec\u001B[38;5;241m.\u001B[39mallow_last_axis_squeeze:\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m!=\u001B[39m spec\u001B[38;5;241m.\u001B[39mndim:\n\u001B[0;32m--> 186\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    187\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    188\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis incompatible with the layer: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    189\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected ndim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, found ndim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    190\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFull shape received: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    191\u001B[0m         )\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec\u001B[38;5;241m.\u001B[39mmax_ndim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m ndim \u001B[38;5;241m>\u001B[39m spec\u001B[38;5;241m.\u001B[39mmax_ndim:\n",
      "\u001B[0;31mValueError\u001B[0m: Input 0 of layer \"lstm_15\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 256)"
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Evaluate the model\n",
    "test_results = model.evaluate(x_test, y_test)\n",
    "test_loss = test_results\n",
    "print(f'Test Loss: {test_loss}')"
   ],
   "id": "35b9916a1a7167c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Reshape y_test from (390, 5, 1) to (390, 5)\n",
    "y_test_reshaped = y_test.reshape(y_test.shape[0], y_test.shape[1])\n",
    "\n",
    "# Inverse transform predictions and y_test\n",
    "predictions_unscaled = scaler.inverse_transform(predictions)\n",
    "y_test_unscaled = scaler.inverse_transform(y_test_reshaped)\n",
    "\n",
    "# Plot predictions vs. true values\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot for each of the 5 prediction days\n",
    "for i in range(5):  # 5 prediction steps\n",
    "    plt.plot(\n",
    "        range(len(predictions_unscaled[:, i])),\n",
    "        predictions_unscaled[:, i],\n",
    "        label=f'Predicted Day {i + 1}',\n",
    "    )\n",
    "    plt.plot(\n",
    "        range(len(y_test_unscaled[:, i])),\n",
    "        y_test_unscaled[:, i],\n",
    "        linestyle='--',\n",
    "        label=f'True Day {i + 1}',\n",
    "    )\n",
    "\n",
    "plt.ylabel('Close Price')\n",
    "plt.xlabel('Days')\n",
    "plt.title('Predicted vs True Close Price Over 5-Day Horizon')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "27515863267a1b9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot predictions and true values in separate subplots\n",
    "plt.figure(figsize=(14, 20))\n",
    "\n",
    "start = 0\n",
    "end = 100\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):  # 5 prediction steps\n",
    "    plt.subplot(5, 1, i + 1)  # Create 5 subplots, one for each prediction day\n",
    "    plt.plot(\n",
    "        range(start, end),\n",
    "        predictions_unscaled[start:end, i],\n",
    "        label=f'Predicted Day {i + 1}',\n",
    "    )\n",
    "    plt.plot(\n",
    "        range(start, end),\n",
    "        y_test_unscaled[start:end, i],\n",
    "        linestyle='--',\n",
    "        label=f'True Day {i + 1}',\n",
    "    )\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xlabel('Days')\n",
    "    plt.title(f'Prediction for Day {i + 1}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a094482d1602f0a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Save the model to hopsworks\n",
    "import hopsworks\n",
    "import os\n",
    "import warnings\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n"
   ],
   "id": "9ef5cf1097237378",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('./project_api_key.txt', 'r') as file:\n",
    "    os.environ[\"HOPSWORKS_API_KEY\"] = file.read().rstrip()\n",
    "    print(\"API Key is set\")\n",
    "    print(\"First 5 characters of API Key: \", os.environ[\"HOPSWORKS_API_KEY\"][:5])"
   ],
   "id": "707a00bc2468933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_dir = \"FinanceModel\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "images_dir = model_dir + \"/images\"\n",
    "if not os.path.exists(images_dir):\n",
    "    os.mkdir(images_dir)"
   ],
   "id": "de3e38302d824a63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#Get loss of model \n",
   "id": "27e595c9141fa64e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "project = hopsworks.login() \n",
    "\n",
    "model_dir = \"Resources/Models\"\n",
    "\n",
    "# Creating input and output schemas using the 'Schema' class for features (X) and target variable (y)\n",
    "input_schema = Schema(x_train)\n",
    "output_schema = Schema(y_train)\n",
    "\n",
    "# Creating a model schema using 'ModelSchema' with the input and output schemas\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "# Converting the model schema to a dictionary representation\n",
    "schema_dict = model_schema.to_dict()\n",
    "model.save(model_dir + \"/model.keras\") \n",
    "\n",
    "#Save the scalers as well\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler, model_dir + \"/scaler.pkl\") \n",
    "\n",
    "\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "#Get model metrics\n",
    "\n",
    "res_dict = {\n",
    "    \"MSE\" : test_loss\n",
    "}\n",
    "\n",
    "aq_model = mr.python.create_model(\n",
    "    name=\"FinanceModel\", \n",
    "    metrics= res_dict,\n",
    "    model_schema=model_schema,\n",
    "    description=\"Predicting stock prices using LSTM model trained on multiple indexes.\",\n",
    ")\n",
    "\n",
    "# Saving the model artifacts to the 'air_quality_model' directory in the model registry\n",
    "aq_model.save(model_dir)\n"
   ],
   "id": "71c3a1d19ba9d708",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "15d138c1e63db812",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
